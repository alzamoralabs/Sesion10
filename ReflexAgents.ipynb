{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f155d1",
   "metadata": {},
   "source": [
    "# Simple Reflex Agent\n",
    "Los simple reflex agents son programas que toman decisiones basadas únicamente en la información actual que reciben del entorno (percept), sin considerar el historial de percepciones pasadas. Funcionan siguiendo reglas de tipo \"if-then\" (si-entonces) para reaccionar de manera inmediata a los estímulos, pero no pueden planificar ni aprender de experiencias previas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50f6be6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to copy 'C:\\\\Users\\\\Bo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\venv\\\\scripts\\\\nt\\\\venvlauncher.exe' to 'c:\\\\gitprojects\\\\ses10\\\\venv\\\\Scripts\\\\python.exe'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in .\\venv\\lib\\site-packages (0.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement random (from versions: none)\n",
      "ERROR: No matching distribution found for random\n"
     ]
    }
   ],
   "source": [
    "!python -m venv venv || source venvnb2\\Scripts\\activate\n",
    "!pip install ollama random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50e3ef",
   "metadata": {},
   "source": [
    "En el ejemplo siguiente, se implementa un agente reflexivo simple que toma decisiones basadas únicamente en la percepción actual del entorno.\n",
    "\n",
    "El agente recibe como entrada un diccionario con valores de temperatura y presión, y utiliza un conjunto de reglas de tipo \"si-entonces\" para determinar la acción a realizar (por ejemplo, encender o apagar la calefacción, activar o desactivar una bomba).\n",
    "\n",
    "El agente evalúa cada regla y ejecuta la acción correspondiente si se cumple la condición, sin considerar el historial de percepciones previas ni mantener un modelo interno del entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "115df07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ollama\n",
    "\n",
    "# Reglas de condición-acción\n",
    "reglas = {\n",
    "    \"temperatura < 18\": \"encender calefacción\",\n",
    "    \"temperatura > 24\": \"apagar calefacción\",\n",
    "    \"presión < 30\": \"activar bomba\",\n",
    "    \"presión > 70\": \"desactivar bomba\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c2c1d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para evaluar condiciones simples\n",
    "def cumple_condicion(percepcion, condicion):\n",
    "    variable, operador, valor = condicion.split()\n",
    "    valor = float(valor)\n",
    "    entrada = percepcion.get(variable)\n",
    "\n",
    "    if entrada is None:\n",
    "        return False\n",
    "\n",
    "    if operador == \"<\":\n",
    "        return entrada < valor\n",
    "    elif operador == \">\":\n",
    "        return entrada > valor\n",
    "    elif operador == \"==\":\n",
    "        return entrada == valor\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Simple Reflex Agent\n",
    "def agente_reflejo_simple(percepcion):\n",
    "    for condicion, accion in reglas.items():\n",
    "        if cumple_condicion(percepcion, condicion):\n",
    "            print(f\"Acción local: {accion}\")\n",
    "            return accion\n",
    "\n",
    "    print(\"Sin acción necesaria.\")\n",
    "    return \"sin acción\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "451553a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percepción actual: {'temperatura': 18, 'presión': 42}\n",
      "Sin acción necesaria.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "percepcion_actual = {\n",
    "    \"temperatura\": random.randint(10, 35), #15 in Celsius\n",
    "    \"presión\": random.randint(20, 80) #45 in PSI\n",
    "}\n",
    "print(f\"Percepción actual: {percepcion_actual}\")\n",
    "accion_tomada = agente_reflejo_simple(percepcion_actual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42ed8d",
   "metadata": {},
   "source": [
    "# Model Based Reflex Agent\n",
    "Un Model Based Reflex Agent es un agente que, además de reaccionar a los estímulos actuales (percepts), mantiene un modelo interno (model) del estado del entorno. Esto le permite tomar decisiones considerando no solo la percepción inmediata, sino también información sobre cómo el entorno puede cambiar con el tiempo. Así, puede manejar situaciones donde la percepción actual no es suficiente para decidir la acción correcta, usando su modelo para inferir el estado real del entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c5b78ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in .\\venv\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain_ollama in .\\venv\\lib\\site-packages (0.3.10)\n",
      "Requirement already satisfied: langchain-openai in .\\venv\\lib\\site-packages (0.3.35)\n",
      "Requirement already satisfied: python-dotenv in .\\venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in .\\venv\\lib\\site-packages (from langchain) (0.3.78)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in .\\venv\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in .\\venv\\lib\\site-packages (from langchain) (0.4.33)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in .\\venv\\lib\\site-packages (from langchain) (2.12.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in .\\venv\\lib\\site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in .\\venv\\lib\\site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in .\\venv\\lib\\site-packages (from langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in .\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in .\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in .\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in .\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in .\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in .\\venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in .\\venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in .\\venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in .\\venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in .\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in .\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in .\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in .\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in .\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in .\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in .\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in .\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in .\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in .\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.5.3 in .\\venv\\lib\\site-packages (from langchain_ollama) (0.6.0)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.104.2 in .\\venv\\lib\\site-packages (from langchain-openai) (2.2.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in .\\venv\\lib\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in .\\venv\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in .\\venv\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in .\\venv\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in .\\venv\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in .\\venv\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.9.18)\n",
      "Requirement already satisfied: colorama in .\\venv\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.104.2->langchain-openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_ollama langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27123713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "# Configuración del LLM local con Ollama y modelo llama3.2\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "system_prompt = \"\"\"Eres asistente inteligente de banca y finanzas.\n",
    "Que decide la mejor acción en base a el estado actual de tu cliente.\n",
    "Las acciones que puedes tomar son:\n",
    "- mostrar saldo de cuenta ** esta no requiere confirmación del cliente **\n",
    "- transferir dinero ** esta requiere confirmación del cliente **\n",
    "- pagar factura ** esta requiere confirmación del cliente **\n",
    "- invertir ** esta requiere confirmación del cliente **\n",
    "** cuando estes a punto de realizar una acción, primero pregunta si el cliente desea proceder con la acción. De esa forma evitas acciones no deseadas y recoges el feedback del cliente. **\n",
    "Tomas en cuenta tu historial de de tus acciones y confirmas si tu cliente decidio proceder o no con la accion que sugieras.\n",
    "- cuando identifiques la accion a tomar, responde ademas con la accion en minusculas y sin puntuacion. Si no hay ninguna accion a tomar, responde \"ninguna\".\n",
    "    por ejemplo:\n",
    "    Accion tomada por agente: mostrar saldo de cuenta\n",
    "- cuando identifiques el feedback del cliente, responde ademas con el feedback en minusculas y sin puntuacion.\n",
    "    Por ejemplo:\n",
    "    Feedback del cliente: si, deseo proceder con la accion\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7e1d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "271ac48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def search_json_in_text(text):\n",
    "    # Expresión regular para encontrar bloques JSON\n",
    "    pattern = r'\\{.*?\\}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return json.loads(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "state_map = {}\n",
    "\n",
    "class State:\n",
    "    def __init__(self, query, status):\n",
    "        self.input = user_query\n",
    "        self.action = agent_action\n",
    "        self.feedback = user_feedback\n",
    "\n",
    "feedback_prompt = (\n",
    "            \"\"\"Analiza el historico de conversacion: {history}\n",
    "            Ahora devuelve siguientes campos y sus valores en formato JSON: 'query_usuario', 'accion_sugerida', 'feedback_usuario'.\n",
    "            \"\"\"\n",
    "        )\n",
    "feedback_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(feedback_prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"\"),\n",
    "])\n",
    "\n",
    "feedback_pipeline = feedback_prompt | llm \n",
    "\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in state_map:\n",
    "        # Si no existe, crea un nuevo historial de chat en memoria\n",
    "        state_map[session_id] = InMemoryChatMessageHistory()\n",
    "        return state_map[session_id]\n",
    "    response = feedback_pipeline.invoke({\"history\": state_map[session_id].messages})\n",
    "    #mydatajson = search_json_in_text(str(response))\n",
    "    state_map[session_id].add_ai_message(str(response))\n",
    "    return state_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c28626b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59b52af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mostrar saldo de cuenta\n",
      "\n",
      "¿desea proceder con esta acción?\n"
     ]
    }
   ],
   "source": [
    "reply = pipeline_with_history.invoke(\n",
    "    {\"query\": \"hola, cual es mi saldo actual?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc13da4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: mostrar saldo de cuenta \n",
      "\n",
      "¿desea proceder con esta acción?\n"
     ]
    }
   ],
   "source": [
    "reply = pipeline_with_history.invoke(\n",
    "    {\"query\": \"Si, solo ver mi saldo\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "802d76d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: invertir 5000 dolares en acciones tecnologicas\n"
     ]
    }
   ],
   "source": [
    "reply = pipeline_with_history.invoke(\n",
    "    {\"query\": \"Quiero invertir 5000 dolares en acciones tecnológicas.\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56901b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para proporcionar el mejor servicio posible a tu cliente, necesito tomar en cuenta su historial de acciones y responder de manera adecuada.\n",
      "\n",
      "Dado que el cliente ha solicitado invertir $5000 en acciones tecnológicas, considerando su comportamiento anterior, no veo la acción necesaria para proceder con esta solicitud. El historial anterior muestra una acción para \"mostrar saldo de cuenta\" pero no proporciona suficiente información sobre cómo está utilizando sus fondos.\n",
      "\n",
      "Por lo tanto, mi respuesta es:\n",
      "ninguna\n"
     ]
    }
   ],
   "source": [
    "reply = pipeline_with_history.invoke(\n",
    "    {\"query\": \"Si, procede.\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57a34148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: hola, cual es mi saldo actual?\n",
      "ai: mostrar saldo de cuenta\n",
      "\n",
      "¿desea proceder con esta acción?\n",
      "ai: Aquí tienes un ejemplo de código en Python que genera un JSON estructurado según tus especificaciones:\n",
      "\n",
      "```python\n",
      "import json\n",
      "\n",
      "class Message:\n",
      "    def __init__(self, content, additional_kwargs=None, response_metadata=None):\n",
      "        self.content = content\n",
      "        self.additional_kwargs = additional_kwargs if additional_kwargs else {}\n",
      "        self.response_metadata = response_metadata if response_metadata else {}\n",
      "\n",
      "class HumanMessage(Message):\n",
      "    pass\n",
      "\n",
      "class AIMessage(Message):\n",
      "    def __init__(self, content, additional_kwargs=None, response_metadata=None):\n",
      "        super().__init__(content, additional_kwargs, response_metadata)\n",
      "        self.accion_sugerida = \"mostrar saldo de cuenta\"\n",
      "\n",
      "def generar_resumen(historico_conversacion):\n",
      "    resumen = {\n",
      "        'query_usuario': \"hola, cual es mi saldo actual?\",\n",
      "        'accion_sugerida': 'mostrar saldo de cuenta',\n",
      "        'feedback_usuario': \"\"\n",
      "    }\n",
      "    \n",
      "    # Iterar por el historico de conversacion\n",
      "    for mensaje in historic_conversacion:\n",
      "        if isinstance(mensaje, HumanMessage):\n",
      "            resumen['query_usuario'] = mensaje.content\n",
      "        elif isinstance(mensaje, AIMessage):\n",
      "            resumen['accion_sugerida'] = mensaje.accion_sugerida\n",
      "\n",
      "    return json.dumps(resumen)\n",
      "\n",
      "# Ejemplo de uso\n",
      "historico_conversacion = [\n",
      "    HumanMessage(content='hola, cual es mi saldo actual?', additional_kwargs={}, response_metadata={}),\n",
      "    AIMessage(content='mostrar saldo de cuenta\\n\\n¿desea proceder con esta acción?', additional_kwargs={}, response_metadata={})\n",
      "]\n",
      "\n",
      "resumen = generar_resumen(historico_conversacion)\n",
      "print(resumen)  # Salida: {\"query_usuario\": \"hola, cual es mi saldo actual?\", \"accion_sugerida\": \"mostrar saldo de cuenta\", \"feedback_usuario\": \"\"}\n",
      "```\n",
      "\n",
      "Espero que esto te sea útil. Si tienes alguna pregunta adicional o necesitas más ayuda, no dudes en preguntar.\n",
      "human: Si, solo ver mi saldo\n",
      "ai: AI: mostrar saldo de cuenta \n",
      "\n",
      "¿desea proceder con esta acción?\n",
      "ai: ```json\n",
      "{\n",
      "    \"query_usuario\": \"Si, solo ver mi saldo\",\n",
      "    \"accion_sugerida\": \"mostrar saldo de cuenta\",\n",
      "    \"feedback_usuario\": \"\"\n",
      "}\n",
      "```\n",
      "\n",
      "Este es el resultado esperado del llamado al método `generar_resumen` con el historico de conversacion proporcionado. El JSON se ha generado según las especificaciones y se contiene los campos 'query_usuario', 'accion_sugerida' y 'feedback_usuario'.\n",
      "human: Quiero invertir 5000 dolares en acciones tecnológicas.\n",
      "ai: AI: invertir 5000 dolares en acciones tecnologicas\n",
      "ai: ```json\n",
      "{\n",
      "    \"query_usuario\": \"Quiero invertir 5000 dolares en acciones tecnológicas.\",\n",
      "    \"accion_sugerida\": \"invertir 5000 dolares en acciones tecnologicas\",\n",
      "    \"feedback_usuario\": \"\"\n",
      "}\n",
      "```\n",
      "\n",
      "Este resultado se obtiene al llamar al método `generar_resumen` con el historico de conversacion proporcionado, que contiene tres mensajes: un mensaje humano que pide información sobre su saldo actual, dos mensajes AI que le preguntan si desea proceder con una acción y cómo está utilizando sus fondos. El resultado es un JSON estructurado con los campos 'query_usuario', 'accion_sugerida' y 'feedback_usuario'.\n",
      "human: Si, procede.\n",
      "ai: Para proporcionar el mejor servicio posible a tu cliente, necesito tomar en cuenta su historial de acciones y responder de manera adecuada.\n",
      "\n",
      "Dado que el cliente ha solicitado invertir $5000 en acciones tecnológicas, considerando su comportamiento anterior, no veo la acción necesaria para proceder con esta solicitud. El historial anterior muestra una acción para \"mostrar saldo de cuenta\" pero no proporciona suficiente información sobre cómo está utilizando sus fondos.\n",
      "\n",
      "Por lo tanto, mi respuesta es:\n",
      "ninguna\n"
     ]
    }
   ],
   "source": [
    "for msg in state_map[\"id_123\"].messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
